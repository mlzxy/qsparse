{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7644f864-63a2-4d04-a594-9af43030f5d9",
   "metadata": {},
   "source": [
    "# Advanced Usage\n",
    "\n",
    "Here we will introduce some advanced usage of QSPARSE by topics. More information can be found at API Reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0ef854-03ac-4674-9f27-03a1e9bf6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler()])\n",
    "from qsparse import set_qsparse_options\n",
    "set_qsparse_options(log_on_created=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d4fd7-6870-41ff-9110-5ed023fb338a",
   "metadata": {},
   "source": [
    "## Layerwise Pruning \n",
    "\n",
    "The function `devise_layerwise_pruning_schedule` will traverse all `pruning operator` throughout the network from input and assign the step for each operator to be activated, to ensure that each pruning operator is activated after all its preceding layers are pruned. The motivation and algorithm details can be found in our MDPI publication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5147638a-c0d8-4940-af49-caeb70a514a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPruning stops at iteration - 23\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): PruneLayer(sparsity=0.5, start=1, interval=10, repetition=1, dimensions={1})\n",
       "  (2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (3): PruneLayer(sparsity=0.5, start=12, interval=10, repetition=1, dimensions={1})\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from qsparse.sparse import prune, devise_layerwise_pruning_schedule\n",
    "\n",
    "\n",
    "net = nn.Sequential(nn.Conv2d(3, 3, 3), \n",
    "                    prune(sparsity=0.5),  # no need to specify `start, repetition, interval`\n",
    "                    nn.Conv2d(3, 3, 3), \n",
    "                    prune(sparsity=0.5))\n",
    "\n",
    "devise_layerwise_pruning_schedule(net, start=1, interval=10) # notice the `start` of each prune layer increases "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f60427-638a-45d3-b139-d7d39da6a732",
   "metadata": {},
   "source": [
    "## Network Conversion\n",
    "\n",
    "The function `convert` comes in handy in producing pruned and quantized network instance without touching the existing floating-point network implementation. Here we introduce some frequent usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4e363-b577-4196-a24a-b7351cb5b6e0",
   "metadata": {},
   "source": [
    "### 1. Inserting pruning operator after all ReLU layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a15776-3881-45f1-8bfd-bc623c5d19a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply `prunesparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1}` on the .first_half.1 activation\n",
      "Apply `prunesparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1}` on the .second_half.1 activation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (first_half): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): PruneLayer(sparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1})\n",
       "    )\n",
       "  )\n",
       "  (second_half): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): PruneLayer(sparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1})\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from qsparse import convert, quantize, prune\n",
    "\n",
    "net = nn.Sequential(OrderedDict([\n",
    "        (\"first_half\", nn.Sequential(nn.Conv2d(3, 3, 3), nn.ReLU())),\n",
    "        (\"second_half\", nn.Sequential(nn.Conv2d(3, 3, 3), nn.ReLU()))]))\n",
    "\n",
    "convert(net, prune(sparsity=0.5), activation_layers=[nn.ReLU], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337a13c-4b38-4104-9ddf-59935737bd2d",
   "metadata": {},
   "source": [
    "### 2. Applying the quantization operator on the weight of all Conv2D layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c457be1a-62c0-4ea8-865f-6b15f321a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply `quantizebits=4, timeout=1000, callback=scalerquantizer, channelwise=1` on the .first_half.0 weight\n",
      "Apply `quantizebits=4, timeout=1000, callback=scalerquantizer, channelwise=1` on the .second_half.0 weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (first_half): Sequential(\n",
       "    (0): Conv2d(\n",
       "      3, 3, kernel_size=(3, 3), stride=(1, 1)\n",
       "      (quantize): QuantizeLayer(bits=4, timeout=1000, callback=ScalerQuantizer, channelwise=1)\n",
       "    )\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (second_half): Sequential(\n",
       "    (0): Conv2d(\n",
       "      3, 3, kernel_size=(3, 3), stride=(1, 1)\n",
       "      (quantize): QuantizeLayer(bits=4, timeout=1000, callback=ScalerQuantizer, channelwise=1)\n",
       "    )\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(net, quantize(bits=4), weight_layers=[nn.Conv2d], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815a668-5d99-4626-8901-e6450bd2d633",
   "metadata": {},
   "source": [
    "### 3. Applying (1) and (2), but excluding the last ReLU and the first Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb80268a-efde-4752-8f07-3ea0e6ff030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply `prunesparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1}` on the .first_half.1 activation\n",
      "Exclude .second_half.1 activation\n",
      "Exclude .first_half.0 weight\n",
      "Apply `quantizebits=4, timeout=1000, callback=scalerquantizer, channelwise=1` on the .second_half.0 weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (first_half): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): PruneLayer(sparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1})\n",
       "    )\n",
       "  )\n",
       "  (second_half): Sequential(\n",
       "    (0): Conv2d(\n",
       "      3, 3, kernel_size=(3, 3), stride=(1, 1)\n",
       "      (quantize): QuantizeLayer(bits=4, timeout=1000, callback=ScalerQuantizer, channelwise=1)\n",
       "    )\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(convert(net, prune(sparsity=0.5), activation_layers=[nn.ReLU], \n",
    "                excluded_activation_layer_indexes=[(nn.ReLU, [-1])], inplace=False), \n",
    "        quantize(bits=4), weight_layers=[nn.Conv2d],\n",
    "        excluded_weight_layer_indexes=[(nn.Conv2d, [0])], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207517dd-6339-4765-aa9c-3fde33f5821e",
   "metadata": {},
   "source": [
    "### 4. Only insert pruning at the first half of the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68333abe-59ba-4790-bb5c-2932be58a713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply `prunesparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1}` on the .first_half.1 activation\n",
      "Exclude .second_half.1 activation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (first_half): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): PruneLayer(sparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1})\n",
       "    )\n",
       "  )\n",
       "  (second_half): Sequential(\n",
       "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(net, prune(sparsity=0.5), activation_layers=[nn.ReLU], include=['first'], inplace=False)\n",
    "# or convert(net, prune(sparsity=0.5), activation_layers=[nn.ReLU], exclude=['second'], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d3a79d-b2b6-4032-a4dc-1e8545c6d195",
   "metadata": {},
   "source": [
    "### 5. Inserting pruning operator before all Conv2D layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3100e996-2374-49ba-95a0-29283dbf3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply `prunesparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1}` on the .first_half.0 activation\n",
      "Apply `prunesparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1}` on the .second_half.0 activation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (first_half): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): PruneLayer(sparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1})\n",
       "      (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (second_half): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): PruneLayer(sparsity=0.5, start=1000, interval=1000, repetition=4, dimensions={1})\n",
       "      (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(net, prune(sparsity=0.5), activation_layers=[nn.Conv2d], order=\"pre\", inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e797e5a-5d08-47c9-b2c4-dba51925860f",
   "metadata": {},
   "source": [
    "## More Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292f4e7-2467-4ca9-b22f-600d8125516a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Symmetric Quantization with Scaler\n",
    "\n",
    "The class `ScalerQuantizer` implements the algorithm 3 in our MDPI paper. Similarly, the class `DecimalQuantizer` shares the exact same implementation except the scaling factor is always restricted to be a power of 2. Their instances can be passed to the `callback` argument of `quantize`, like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6eddd1f-efc6-4b26-b4c4-26bae29f3f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizeLayer(bits=8, timeout=1000, callback=DecimalQuantizer, channelwise=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qsparse.quantize import DecimalQuantizer\n",
    "\n",
    "quantize(bits=8, callback=DecimalQuantizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cbcc1-8129-4223-b098-4d62e3a4e2be",
   "metadata": {},
   "source": [
    "The `ScalerQuantizer` and `DecimalQuantizer` includes the functions of both inference and parameters learning. To access only the inference function to quantize tensors, one can use functions `quantize_with_scaler` and `quantize_with_decimal`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "143085e9-941b-4a43-8e9f-e23ae79b913c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8.4718e-05), tensor(8.5786e-06))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from qsparse.quantize import quantize_with_decimal, quantize_with_scaler\n",
    "\n",
    "data = torch.rand(1000)\n",
    "\n",
    "((data - quantize_with_decimal(data, bits=8, decimal=6))**2).mean(), ((data - quantize_with_scaler(data, bits=8, scaler=0.01))**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2760f-4318-4e53-a3bc-1767cf77e7b5",
   "metadata": {},
   "source": [
    "### Asymmetric Quantization\n",
    "\n",
    "The class `AdaptiveQuantizer` implements the algorithm 2 in our MDPI paper, which estimates the lower and upper bounds of incoming data streams and apply assymmetric quantization. Its inference function can be accessed from `quantize_with_line`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8abd6c2f-ed63-4334-899a-5fb6f1fb4839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizeLayer(bits=8, timeout=1000, callback=AdaptiveQuantizer, channelwise=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qsparse.quantize import AdaptiveQuantizer\n",
    "\n",
    "quantize(bits=8, callback=AdaptiveQuantizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ec00369-d0ff-4c51-9435-7099a7fca05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3243e-06)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qsparse.quantize import quantize_with_line\n",
    "\n",
    "((data - quantize_with_line(data, bits=8, lines=(0, 1)))**2).mean() # lines specify the (lower, upper) bounds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933ce74-a784-4530-82f2-620121c36116",
   "metadata": {},
   "source": [
    "### Channelwise Quantization\n",
    "\n",
    "Channel-wise quantization denotes the technique to use different decimal bits cross different channels, i.e., quantize each channel independently. It is commonly known that channel-wise quantization can reduce quantization error drastically especially when inter-channel numerical ranges have large variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09047c89-d206-40d2-95bf-f886b4281bc0",
   "metadata": {},
   "source": [
    "To specify channelwise quantization on dimension 1 (dimension 1 as channel): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2503bf7d-f5ff-45cb-a9c8-eae445eb2397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizeLayer(bits=8, timeout=1000, callback=ScalerQuantizer, channelwise=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize(bits=8, channelwise=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d5316e-ff4b-4f93-bf74-b67d1fbc0aa9",
   "metadata": {},
   "source": [
    "To disable channelwise quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98c6a661-cbb2-412f-b84a-1f89c4574ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizeLayer(bits=8, timeout=1000, callback=ScalerQuantizer, channelwise=-1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize(bits=8, channelwise=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31f204-165a-497c-8192-6862f1e8766c",
   "metadata": {},
   "source": [
    "### Groupwise Quantization\n",
    "\n",
    "Channelwise quantization allocates one set of scaling factor and zero-point for each channel, which could possibly complicate the inference implementation when both weight and activations are quantized channel-wisely, especially for networks with a large number of channels. Here, we provide a technique, which we name as _groupwise quantization_. Specifically, we cluster the channel-wise quantization parameters (scaling factor and zero-points) into groups, and share one set of quantization parameter within each group. We empirically find that groupwise quantization yields little to no performance drop compared to channelwise pruning, even with an extremely small group number, e.g. 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "316f1251-92df-4fc5-afc1-b03622a0da7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mquantizing  with 8 bits\u001b[0m\n",
      "\u001b[31mclustering 1024 channels into 4 groups\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layer = quantize(bits=8, channelwise=1, \n",
    "                 callback=DecimalQuantizer(group_num=4, \n",
    "                                   # `group_timeout` denotes the steps when the clustering starts after the activation of the quantization operator. \n",
    "                                   group_timeout=10), timeout=10)\n",
    "for _ in range(21):\n",
    "    layer(torch.rand(1, 1024, 3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85962877-c877-40d1-9eac-87ac7f3fc06b",
   "metadata": {},
   "source": [
    "For a convolution layer with 1024 channels, using groupwise quantization with 4 groups produces a 256 times of reduction in the number of quantization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292de77-7e5b-4402-93d6-713407c7d53b",
   "metadata": {},
   "source": [
    "### Quantization Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7920b51-90ef-48bd-8104-79f7f4d426a8",
   "metadata": {},
   "source": [
    "By default, for weight quantization, quantize will only quantize the weight parameter and leave the bias parameter to have full precision [(Jacob et al.)](https://arxiv.org/abs/1712.05877). The reason is that bias can be used to initialize the high precision accumulator for the mult-add operations. Bias can be quantized in QSPARSE by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d97ccf-382f-4de5-aabd-06f949def041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(\n",
       "  1, 1, kernel_size=(1, 1), stride=(1, 1)\n",
       "  (quantize): QuantizeLayer(bits=8, timeout=1000, callback=ScalerQuantizer, channelwise=1)\n",
       "  (quantize_bias): QuantizeLayer(bits=12, timeout=1000, callback=ScalerQuantizer, channelwise=0)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qsparse import quantize\n",
    "\n",
    "quantize(nn.Conv2d(1, 1, 1), bits=8, bias_bits=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c25ed-bf98-4e2e-909d-310c354e88f5",
   "metadata": {},
   "source": [
    "### Integer Arithmetic Verification\n",
    "\n",
    "Here we provide an example to demonstrate floating-point simulated quantization can fully match with 8-bit integer arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf2a4f30-d128-4df4-a089-e0457f3fb352",
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = 7 # input shift\n",
    "no = 6 # output shift\n",
    "\n",
    "input = torch.randint(-128, 127, size=(3, 10, 32, 32))\n",
    "input_float = input.float() / 2 ** ni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d14e2d5-6e46-4d30-a059-c99b5ffda356",
   "metadata": {},
   "source": [
    "Quantization computation simulated with floating-point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43eb389d-033b-4be0-af94-78409e783e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mquantizing  with 8 bits\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "timeout = 5\n",
    "qconv = quantize(\n",
    "    torch.nn.Conv2d(10, 30, 3, bias=False), bits=8, timeout=timeout, channelwise=0, callback=DecimalQuantizer()\n",
    ") \n",
    "qconv.train()\n",
    "for _ in range(timeout + 1):  # ensure the quantization has been triggered\n",
    "    qconv(input_float)\n",
    "output_float = quantize_with_decimal(qconv(input_float), 8, no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123671a7-a188-45ca-8bea-1bd5e6f8122f",
   "metadata": {},
   "source": [
    "Reproduce the above computation in 8-bit arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00c5ec40-78ab-4c20-a984-ac672b31fb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully match with integer arithmetic\n"
     ]
    }
   ],
   "source": [
    "decimal = (1 / qconv.quantize.weight).nan_to_num(posinf=1, neginf=1).log2().round().int()\n",
    "weight = qconv.weight * (2.0 ** decimal).view(-1, 1, 1, 1)\n",
    "output_int = F.conv2d(input.int(), weight.int())\n",
    "for i in range(output_int.shape[1]):\n",
    "    output_int[:, i] = (\n",
    "        output_int[:, i].float() / 2 ** (ni + decimal[i] - no)\n",
    "    ).int()\n",
    "\n",
    "diff = (\n",
    "    output_float.detach().numpy() - (output_int.float() / 2 ** no).detach().numpy()\n",
    ")\n",
    "assert np.all(diff == 0)\n",
    "\n",
    "print(\"Fully match with integer arithmetic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09074314-59d1-4389-8810-f6404259593c",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35985a50-fc90-4157-a468-527a09c5c806",
   "metadata": {},
   "source": [
    "### Resuming from Checkpoint \n",
    "\n",
    "Both `quantize` and `prune` layers support to resume training from a checkpoint. However, due to the fact that: \n",
    "\n",
    "1. QSPARSE determines the shape of its parameters (e.g. `scaling factor`, `mask`) at the first forward pass.\n",
    "2. `load_state_dict` currently does not allow shape mismatch ([pytorch/issues#40859](https://github.com/pytorch/pytorch/issues/40859))\n",
    "\n",
    "Therefore, we provide the `preload_qsparse_state_dict` to be called before the `load_state_dict` to mitigate the above issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae2b3f8d-27bc-43a8-965d-f967b9918f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[Prune] start = 200 interval = 10 repetition = 4 sparsity = 0.5 dimensions = {1}\u001b[0m\n",
      "[Quantize] bits=8 channelwise=1 timeout=100\n",
      "\u001b[33mquantizing  with 8 bits\u001b[0m\n",
      "\u001b[33m[Prune] [Step 200] pruned 0.29\u001b[0m\n",
      "\u001b[33mStart pruning at  @ 200\u001b[0m\n",
      "\u001b[33m[Prune] [Step 210] pruned 0.44\u001b[0m\n",
      "\u001b[33m[Prune] [Step 220] pruned 0.49\u001b[0m\n",
      "\u001b[33m[Prune] [Step 230] pruned 0.50\u001b[0m\n",
      "\u001b[33m[Prune] start = 200 interval = 10 repetition = 4 sparsity = 0.5 dimensions = {1}\u001b[0m\n",
      "[Quantize] bits=8 channelwise=1 timeout=100\n",
      "\n",
      "Catch error as expected: Error(s) in loading state_dict for Conv2d:\n",
      "\tUnexpected key(s) in state_dict: \"prune.callback.magnitude\", \"quantize.weight\", \"quantize._n_updates\". \n",
      "\tsize mismatch for prune.mask: copying a param with shape torch.Size([1, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([]).\n",
      "\n",
      "\u001b[33m[Prune] start = 200 interval = 10 repetition = 4 sparsity = 0.5 dimensions = {1}\u001b[0m\n",
      "[Quantize] bits=8 channelwise=1 timeout=100\n",
      "successfully loading from checkpoint\n"
     ]
    }
   ],
   "source": [
    "from qsparse.util import preload_qsparse_state_dict\n",
    "\n",
    "def make_conv():\n",
    "    return quantize(prune(nn.Conv2d(16, 32, 3), \n",
    "                        sparsity=0.5, start=200, \n",
    "                        interval=10, repetition=4), \n",
    "                bits=8, timeout=100)\n",
    "\n",
    "conv = make_conv()\n",
    "\n",
    "for _ in range(241):\n",
    "    conv(torch.rand(10, 16, 7, 7))\n",
    "\n",
    "try:\n",
    "    conv2 = make_conv()\n",
    "    conv2.load_state_dict(conv.state_dict())\n",
    "except RuntimeError as e:\n",
    "    print(f'\\nCatch error as expected: {e}\\n' )\n",
    "\n",
    "conv3 = make_conv()\n",
    "preload_qsparse_state_dict(conv3, conv.state_dict())\n",
    "conv3.load_state_dict(conv.state_dict())\n",
    "\n",
    "tensor = torch.rand(10, 16, 7, 7)\n",
    "assert np.allclose(conv(tensor).detach().numpy(), conv3(tensor).detach().numpy(), atol=1e-6)\n",
    "print('successfully loading from checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9701b4c3-68ae-4452-a314-256b6bce2cf5",
   "metadata": {},
   "source": [
    "### Inspecting Parameters of a Pruned/Quantized Model\n",
    "\n",
    "Parameters of a quantized and pruned networks can be easily inspected and therefore post-processed for use cases such as compiling for neural engines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d78f1c84-744e-434b-817c-75887e376d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight (32, 16, 3, 3)\n",
      "bias (32,)\n",
      "prune.mask (1, 16, 1, 1)\n",
      "prune._n_updates (1,)\n",
      "prune._cur_sparsity (1,)\n",
      "prune.callback.t (1,)\n",
      "prune.callback.magnitude (1, 16, 1, 1)\n",
      "quantize.weight (16, 1)\n",
      "quantize._n_updates (1,)\n"
     ]
    }
   ],
   "source": [
    "state_dict = conv.state_dict()\n",
    "for k,v in state_dict.items():\n",
    "    print(k, v.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a77764-63a2-4337-8822-e14d60cfda23",
   "metadata": {},
   "source": [
    "\n",
    "| Param           | Description                                                                  |\n",
    "|-----------------|------------------------------------------------------------------------------|\n",
    "| `quantize.weight` | scaling factors                                     |\n",
    "| `*._n_updates`    | internal counter for number of training steps                                |\n",
    "| `prune.mask`            | binary mask for pruning                                                      |\n",
    "| `prune._cur_sparsity` | internal variable to record current sparsity                                 |\n",
    "| `prune.callback.magnitude`    | internal boolean variable to record whether quantization has been triggered. |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
