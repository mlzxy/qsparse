{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Usage\n",
    "\n",
    "Here we will introduce some advanced usage of QSPARSE by topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler()])\n",
    "from qsparse import set_qsparse_options\n",
    "set_qsparse_options(log_on_created=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel-wise Quantization\n",
    "\n",
    "Channel-wise quantization denotes the technique to use different decimal bits cross different channels, i.e., quantize each channel independently. It is commonly known that channel-wise quantization can reduce quantization error drastically especially when inter-channel numerical ranges have large variance. By default, `quantize` will apply channel-wise quantization along the dimension of index `1`, and it can be configured through the `channelwise` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from qsparse import quantize\n",
    "tensor = torch.rand(10, 20, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize_layer = quantize(bits=8)\n",
    "quantize_layer(tensor)\n",
    "quantize_layer.decimal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize_layer = quantize(bits=8, channelwise=2) # quantize along dimension 2\n",
    "quantize_layer(tensor)\n",
    "quantize_layer.decimal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize_layer = quantize(bits=8, channelwise=-1) # disable channel-wise quantization\n",
    "quantize_layer(tensor)\n",
    "quantize_layer.decimal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various inference engines support different schemes of channel-wise quantization, which can also vary according to types of layers, e.g. [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) and [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) are different on the weight layout. The `channelwise` parameter can be used to adjust the network training to match the inference setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Quantization\n",
    "\n",
    "By default, for weight quantization, `quantize` will only quantize the `weight` parameter and leave the `bias` parameter to have full precision ([Jacob et al.](https://arxiv.org/abs/1712.05877)). The reason is that bias can be used to initialize the high precision accumulator for the mult-add operations. Bias can be quantized in QSPARSE by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(\n",
       "  1, 1, kernel_size=(1, 1), stride=(1, 1)\n",
       "  (quantize): QuantizeLayer()\n",
       "  (quantize_bias): QuantizeLayer()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from qsparse import quantize\n",
    "\n",
    "quantize(nn.Conv2d(1, 1, 1), bits=8, bias_bits=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturated Quantization\n",
    "\n",
    "When quantizing generative adversarial networks (GAN), we found the estimation of the optimal decimal bits $d^*$ can be sensitive to outliers in the activation distribution, which leads to an under-representation of quantized values.\n",
    "\n",
    "\n",
    "<figure style=\"text-align:center;font-style:italic\"> \n",
    "  <img src=\"../docs/assets/long-tail.jpg\" />\n",
    "  <figcaption>Long-tailed distribution in the activation space of GANs.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We discover that this under-representation issue can be simply and effectively moderated by clipping the outliers. Let hyper parameters $q_l, q_u$ denote the lower and upper quantile of the activation distribution, we can calculate the optimal bits $d^*$ by:\n",
    "\n",
    "$$\n",
    "d^* = \\arg \\min_{d} \\Vert Q_u(\\mathbf{x}_{t_q}, d) - S_{q_l, q_u}(\\mathbf{x}_{t_q})\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{q_l, q_u}(\\mathbf{x}) = \\text{clip}(\\mathbf{x}, \\small{\\text{quantile}}(\\mathbf{x}, q_l), \\small{\\text{quantile}}(\\mathbf{x}, q_u)) \n",
    "$$\n",
    "\n",
    "This can be implemented in QSPARSE by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 1), (0.0001, 0.9999))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qsparse import quantize\n",
    "\n",
    "# saturate_range=(q_u, q_l)\n",
    "# by default, saturate_range = (0, 1), means no saturation is applied\n",
    "(quantize(bits=8).saturate_range, \n",
    " quantize(bits=8, saturate_range=(0.0001, 0.9999)).saturate_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Pruning\n",
    "\n",
    "Structure pruning denotes the technique to enforce a topology (e.g. pruning over certain dimension) over the binary mask created through pruning procedure. By default, `prune` will apply unstructured pruning, but it supports dimension-wise structured pruning through following configuration:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from qsparse import prune, structured_prune_callback\n",
    "tensor = torch.rand(10, 20, 30, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 30, 40])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pruning on all dimensions\n",
    "quantize_layer = prune(sparsity=0.5, collapse=-1) \n",
    "quantize_layer(tensor)\n",
    "quantize_layer.mask.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only prune on the dimension 0, 1\n",
    "quantize_layer = prune(sparsity=0.5, collapse=-1,\n",
    "    callback=lambda *args: structured_prune_callback(*args, prunable={0, 1})) \n",
    "quantize_layer(tensor)\n",
    "quantize_layer.mask.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `callback` parameter can be used to extended the functionalities of both `quantize` and `prune`, which we will revisit with details in [Extending Methods of Quantization And Pruning](#extending-methods-of-quantization-and-pruning). The `collapse` parameter is used to configure which dimension to ignore (e.g. batch dimension) when creating binary masks, which is set automatically according to pruning target (i.e. weight or activation). We explicitly set it to `-1` for more clarity in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Size for Activation Pruning\n",
    "\n",
    "Contrary to weights which are static during inference, activations are dynamically conditioned on the input to the network. To create a stable binary mask for activation pruning, we introduce a sliding window technique: \n",
    "\n",
    "$$\n",
    "\\mathbf{M}_{\\mathbf{h}_t,s}(i,j) = \\begin{cases}\n",
    "\t\t1 & \\sum\\limits_{n=0}^{T-1} |\\mathbf{h}_{t-n}(i, j)| \\ge \\text{quantile}(\\sum\\limits_{n=0}^{T-1} |\\mathbf{h}_{t-n}|, s) \\\\\n",
    "\t\t0 & \\text{otherwise}\n",
    "\\end{cases}  \n",
    "$$\n",
    "\n",
    "where $\\mathbf{h}_t$ denote the activations at time $t$ and $T$ denote the size of the sliding window. This can be implemented in QSPARSE by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from qsparse import prune\n",
    "\n",
    "print(prune(sparsity=0.5).window_size) # by default T = 1\n",
    "print(prune(sparsity=0.5, window_size=10).window_size) # T = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change input sizes during evaluation \n",
    "\n",
    "By default, QSPARSE learns a binary mask for each feature map. The shape of the binary mask is identical to the corresponding activation. Therefore, error will occur if we vary the input size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catch error as expect: The expanded size of the tensor (64) must match the existing size (32) at non-singleton dimension 3.  Target sizes: [1, 10, 64, 64].  Tensor sizes: [10, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qsparse import prune\n",
    "data = torch.rand((1, 10, 32, 32))\n",
    "data2x = torch.rand((1, 10, 64, 64))\n",
    "\n",
    "prune_layer = prune(sparsity=0.5)\n",
    "prune_layer(data)\n",
    "prune_layer.eval()\n",
    "try:\n",
    "    prune_layer(data2x)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Catch error as expect: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QSPARSE provides an option to expand the binary mask to matched the input tensor shape during evaluation, which is useful for tasks like super resolution. This option can be enabled by setting `strict` parameter to False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Able to change input shape during evaluation\n"
     ]
    }
   ],
   "source": [
    "prune_layer = prune(sparsity=0.5, strict=False)\n",
    "prune_layer(data)\n",
    "prune_layer.eval()\n",
    "prune_layer(data2x)\n",
    "print('Able to change input shape during evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming from Checkpoint \n",
    "\n",
    "Both `quantize` and `prune` layers support to resume training from a checkpoint. However, due to the fact that: \n",
    "\n",
    "1. QSPARSE determines the shape of its parameters (e.g. `decimal`, `mask`) at the first forward pass.\n",
    "2. `load_state_dict` does not allow shape mismatch ([pytorch/issues#40859](https://github.com/pytorch/pytorch/issues/40859))\n",
    "\n",
    "Therefore, it is necessary to ensure the QSPARSE parameters have been initialized before loading the state dict, which can easily be done with a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Quantize] (channelwise) avg decimal = 8.0\n",
      "[Prune] [Step 210] active 0.72, pruned 0.28, window_size = 1\n",
      "[Prune] [Step 220] active 0.57, pruned 0.43, window_size = 1\n",
      "[Prune] [Step 230] active 0.51, pruned 0.49, window_size = 1\n",
      "[Prune] [Step 240] active 0.50, pruned 0.50, window_size = 1\n",
      "\n",
      "Catch error as expected: Error(s) in loading state_dict for Conv2d:\n",
      "\tsize mismatch for prune.mask: copying a param with shape torch.Size([32, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([]).\n",
      "\n",
      "successfully loading from checkpoint\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from qsparse import quantize, prune\n",
    "\n",
    "def make_conv():\n",
    "    return quantize(prune(nn.Conv2d(1, 32, 3), \n",
    "                        sparsity=0.5, start=200, \n",
    "                        interval=10, repetition=4), \n",
    "                bits=8, timeout=100)\n",
    "\n",
    "conv = make_conv()\n",
    "\n",
    "for _ in range(241):\n",
    "    conv(torch.rand(10, 1, 28, 28))\n",
    "\n",
    "try:\n",
    "    conv2 = make_conv()\n",
    "    conv2.load_state_dict(conv.state_dict())\n",
    "except RuntimeError as e:\n",
    "    print(f'\\nCatch error as expected: {e}\\n' )\n",
    "\n",
    "conv3 = make_conv()\n",
    "conv3(torch.rand(10, 1, 28, 28))\n",
    "conv3.load_state_dict(conv.state_dict())\n",
    "\n",
    "tensor = torch.rand(10, 1, 28, 28)\n",
    "assert torch.all(conv(tensor) == conv3(tensor)).item() == True\n",
    "print('successfully loading from checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Logging\n",
    "\n",
    "QSPARSE will log quantizing and pruning progress into the console. We can use a function `auto_name_prune_quantize_layers` to associate these logs to the specific layer names and therefore increase the transparency of the training program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Quantize @ conv.quantize] (channelwise) avg decimal = 8.125\n",
      "INFO:root:[Quantize @ conv_output_quantization] (channelwise) avg decimal = 7.34375\n",
      "INFO:root:[Prune @ conv.prune] [Step 210] active 0.72, pruned 0.28, window_size = 1\n",
      "INFO:root:[Prune @ conv_output_pruning] [Step 210] active 0.71, pruned 0.29, window_size = 1\n",
      "INFO:root:[Prune @ conv.prune] [Step 220] active 0.57, pruned 0.43, window_size = 1\n",
      "INFO:root:[Prune @ conv_output_pruning] [Step 220] active 0.56, pruned 0.44, window_size = 1\n",
      "INFO:root:[Prune @ conv.prune] [Step 230] active 0.51, pruned 0.49, window_size = 1\n",
      "INFO:root:[Prune @ conv_output_pruning] [Step 230] active 0.51, pruned 0.49, window_size = 1\n",
      "INFO:root:[Prune @ conv.prune] [Step 240] active 0.50, pruned 0.50, window_size = 1\n",
      "INFO:root:[Prune @ conv_output_pruning] [Step 240] active 0.50, pruned 0.50, window_size = 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from qsparse import auto_name_prune_quantize_layers, quantize, prune\n",
    "\n",
    "net = nn.Sequential(\n",
    "    OrderedDict([\n",
    "        ('conv', quantize(prune(nn.Conv2d(1, 32, 3, bias=False), \n",
    "                        sparsity=0.5, start=200, \n",
    "                        interval=10, repetition=4), \n",
    "                    bits=8, timeout=100, channelwise=0)),\n",
    "        ('conv_output_pruning', prune(sparsity=0.5, start=200, interval=10, repetition=4)),\n",
    "        ('conv_output_quantization', quantize(bits=8, timeout=100)),\n",
    "    ])\n",
    ")\n",
    "\n",
    "net = auto_name_prune_quantize_layers(net)\n",
    "\n",
    "for _ in range(241):\n",
    "    net(torch.rand(10, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Parameters of a Pruned/Quantized Model\n",
    "\n",
    "Parameters of a quantized and pruned networks can be easily inspected and therefore post-processed for use cases such as compiling for neural engines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv.weight (32, 1, 3, 3)\n",
      "conv.prune.mask (32, 1, 3, 3)\n",
      "conv.prune._n_updates (1,)\n",
      "conv.prune._cur_sparsity (1,)\n",
      "conv.quantize.decimal (32,)\n",
      "conv.quantize._n_updates (1,)\n",
      "conv.quantize.bits ()\n",
      "conv.quantize._quantized (1,)\n",
      "conv_output_pruning.mask (32, 26, 26)\n",
      "conv_output_pruning._n_updates (1,)\n",
      "conv_output_pruning._cur_sparsity (1,)\n",
      "conv_output_quantization.decimal (32,)\n",
      "conv_output_quantization._n_updates (1,)\n",
      "conv_output_quantization.bits ()\n",
      "conv_output_quantization._quantized (1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9., 8., 8., 8., 8., 8., 8., 8., 9., 8., 8., 8., 8., 8., 8., 8., 8.,\n",
       "       8., 8., 8., 8., 8., 9., 8., 8., 8., 8., 8., 8., 8., 8., 8.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continue from the `net` variable from the previous example\n",
    "state_dict = net.state_dict()\n",
    "for k,v in state_dict.items():\n",
    "    print(k, v.numpy().shape)\n",
    "\n",
    "net.state_dict()['conv.quantize.decimal'].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Param           | Description                                                                  |\n",
    "|-----------------|------------------------------------------------------------------------------|\n",
    "| `mask`          | binary mask for pruning                                                      |\n",
    "| `decimal`       | bits that used to represent fractionals                                      |\n",
    "| `bits`          | total number of bits                                                         |\n",
    "| `_n_updates`    | internal counter for number of training steps                                |\n",
    "| `_cur_sparsity` | internal variable to record current sparsity                                 |\n",
    "| `_quantized`    | internal boolean variable to record whether quantization has been triggered. |\n",
    "\n",
    "\n",
    "### Integer Arithmetic Verification\n",
    "\n",
    "Using the parameters from the previous section, we give an example in the following to implement the computation with 8-bit integer arithmetic and fully match with floating-point results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimal_inp = 7 # input decimal bits\n",
    "input_int = torch.randint(-128, 127, size=(10, 1, 28, 28)).int()\n",
    "input_float = input_int.float() / 2 ** decimal_inp\n",
    "output_float = net(input_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "weight_int = torch.clamp((state_dict['conv.weight'] * \n",
    "            ((2.0 ** state_dict['conv.quantize.decimal']).view(-1, 1, 1, 1))).int(), \n",
    "        -128, 127) * state_dict['conv.prune.mask'].int()\n",
    "\n",
    "output_int_high_precision = F.conv2d(input_int, weight_int)\n",
    "\n",
    "# simulating bit shifting in integer arithmetic computation engine\n",
    "for i in range(output_int_high_precision.shape[1]):\n",
    "    output_int_high_precision[:, i] = (\n",
    "        output_int_high_precision[:, i].float() /\n",
    "         2 ** (decimal_inp + state_dict['conv.quantize.decimal'][i]\n",
    "                 - state_dict['conv_output_quantization.decimal'][i])\n",
    "    ).int()\n",
    "\n",
    "output_int = torch.clamp(output_int_high_precision, min=-128, max=127) * state_dict['conv_output_pruning.mask'].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully match with integer arithmetic!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "diff = (\n",
    "    output_float.detach().numpy() - (output_int.float() / 2 ** state_dict['conv_output_quantization.decimal'].view(1, -1, 1, 1)).detach().numpy()\n",
    ")\n",
    "assert np.all(diff == 0)\n",
    "print('Fully match with integer arithmetic!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending Methods of Quantization And Pruning\n",
    "\n",
    "Both `quantize` and `prune` have the `callback` parameter, which is a function that operates on the tensor level to implement the quantization and pruning operations. Currently, QSPARSE provides the following built-in callbacks.\n",
    "\n",
    "- Quantization:\n",
    "    - [linear_quantize_callback](../reference/quantize/#qsparse.quantize.linear_quantize_callback)\n",
    "- Pruning:\n",
    "    - [unstructured_prune_callback](../reference/sparse/#qsparse.sparse.unstructured_prune_callback)\n",
    "    - [structured_prune_callback](../reference/sparse/#qsparse.sparse.structured_prune_callback)\n",
    "\n",
    "The type signatures for quantization and pruning can be found at [QuantizeCallback](../reference/common/#qsparse.common.QuantizeCallback) and [PruneCallback](../reference/common/#qsparse.common.PruneCallback). The `callback` parameter can be used as an interface to extend QSPARSE.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
