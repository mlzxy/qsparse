{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "In this tutorial, we will give a brief introduction on the quantization and pruning techniques upon which QSPARSE is built. Using our library, we guide you through the building of a image classification neural network, whose both weights and activations are fully quantized and pruned to a given sparsity level.\n",
    "\n",
    "> If you are already familiar with quantization and pruning methods and want to learn the programming syntax, please fast forward to [Building Network with QSPARSE](#building-network-with-qsparse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Quantization and pruning are core techniques used to reduce the inference costs of deep neural networks and have been  studied extensively. Approaches to quantization are often divided into two categories: \n",
    "\n",
    "1. Post-training quantization\n",
    "2. Quantization aware training\n",
    "\n",
    "The former applies quantization after a network has been trained, and the latter quantizes the network during training and thereby reduces the quantization error throughout training process and usually yields superior performance. \n",
    "\n",
    "Pruning techniques are often divided into unstructured or structured approaches which define if and how to impose a pre-defined topology, e.g. channel-wise pruning. \n",
    "\n",
    "Here, we focus on applying quantization and unstructured pruning during training.\n",
    "\n",
    "<figure style=\"text-align:center;font-style:italic\"> \n",
    "  <img src=\"../docs/assets/network_diagram-p1.svg\" />\n",
    "  <figcaption>Conceptual diagram of the computational graph of a network whose weights and activations are quantized and pruned using QSPARSE.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "In QSPARSE, we implement the quantization and pruning as independent operators, which can be applied on both weights and activations, as demonstrated in the figure above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Quantization\n",
    "\n",
    "We denote the uniform quantization operation as $Q_u(\\mathbf{x}, d)$, where $\\mathbf{x}$ denotes the input to the operator  (i.e. weights or activations), $N$ denotes the total number of bits used to represent weights and activations, and $d$ denotes the number of bits used to represent the fractional (i.e. the position of the decimal point to the right, we will refer $d$ as decimal bits).\n",
    "\n",
    "$$\n",
    "Q_u(\\mathbf{x}, d) = \\text{clip}(\\lfloor\\mathbf{x} \\times 2^{d}\\rfloor, -2^{N-1}, 2^{N-1}-1) / 2^d\n",
    "$$\n",
    "\n",
    "Straight-through estimator (STE) is applied to calculate gradients in the backward computation.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial \\mathbf{x}} = \\text{clip}(\\frac{\\partial Loss}{\\partial Q_u(\\mathbf{x}, d)}, -2^{N-d-1}, 2^{N-d-1} - 2^{-d})\n",
    "$$\n",
    "\n",
    "However, STE is known to be sensitive to weight initialization, therefore, we design the quantization operator as $\\text{Quantize}$ in the following. Starting with the original full-precision network, we delay the quantization of the network to later training stages, and calculate the optimal decimal bits $d^*$ by minimizing the quantization error after a given number of update steps $t_q$.\n",
    "\n",
    "$$\n",
    "\\text{Quantize}(\\mathbf{x}_t)  = \\begin{cases} \n",
    "    \\mathbf{x}_t & t < t_q \\\\\n",
    "    Q_u(\\mathbf{x}_t, d^*)  &    t \\ge t_q   \\\\\n",
    "    \\end{cases} \n",
    "$$\n",
    "\n",
    "$$\n",
    "d^* = \\arg \\min_{d} \\Vert Q_u(\\mathbf{x}_{t_q}, d) - \\mathbf{x}_{t_q} \\Vert^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude-based Unstructured Pruning\n",
    "\n",
    "We denote the unstructured pruning operator $\\textbf{Prune}(\\mathbf{x}, s)$ as element-wise multiplication between $\\mathbf{x}$ and $\\mathbf{M}_{\\mathbf{x},s}$, where $\\mathbf{x}$ denotes the input to the operator (i.e., weights or activations), $s$ denotes the target sparsity as measured by the percentage of zero-valued elements, and $\\mathbf{M}_{\\mathbf{x},s}$ denotes a binary mask.\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}, s)  = \\mathbf{x} \\circ \\mathbf{M}_{\\mathbf{x},s}\n",
    "$$\n",
    "\n",
    "Given that $(i,j)$ are the row and column indices, respectively, the binary mask $\\mathbf{M}_{\\mathbf{x},s}$ is calculated as belows, where the $\\text{quantile}(\\mathbf{x}, a)$ is the a-th quantile of $\\mathbf{x}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{M}_{\\mathbf{x},s}^{(i,j)}  = \\begin{cases}\n",
    "\t\t1 & |\\mathbf{x}^{(i, j)}| \\ge \\text{quantile}(|\\mathbf{x}|, s) \\\\\n",
    "\t\t0 & \\text{otherwise}\n",
    "\t\t\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "As proposed by [Zhu et al.](https://arxiv.org/pdf/1710.01878.pdf), the sparsity level $s$ is controlled and updated according to a sparsification schedule at time steps $t_p + i \\Delta t_p$ such that $i \\in \\{1,2,..,,n\\}$, where $t_p$, $\\Delta t_p$, and $n$ are hyper parameters that represent the starting pruning step, frequency, and total number of pruning iterations, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Network with QSPARSE\n",
    "\n",
    "With the above methods in mind, in the following, we will use QSPARSE to build a quantized and sparse network upon the below full precision network borrowed from pytorch official [MNIST example](https://github.com/pytorch/examples/blob/master/mnist/main.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.bn3(self.fc1(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Quantization and Pruning\n",
    "\n",
    "<figure style=\"text-align:center;font-style:italic\"> \n",
    "  <img src=\"../docs/assets/network_diagram-p2.svg\" />\n",
    "  <figcaption>The part of diagram in red corresponds to weight quantization and pruning.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "We can easily create a weight quantized and pruned layer with QSPARSE. Take the convolution as an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(\n",
       "  1, 32, kernel_size=(3, 3), stride=(1, 1)\n",
       "  (prune): PruneLayer()\n",
       "  (quantize): QuantizeLayer()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qsparse import prune, quantize, set_qsparse_options\n",
    "set_qsparse_options(log_on_created=False)\n",
    "\n",
    "conv = quantize(prune(nn.Conv2d(1, 32, 3), \n",
    "                      sparsity=0.5, start=200, \n",
    "                      interval=10, repetition=4), \n",
    "                bits=8, timeout=100)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `prune` and `quantize` layers are injected. The output layer will behave identically to `nn.Conv2d` except that `conv.weight` will return a quantized and pruned version of the vanilla weight. As for the hyper parameters, they map to QSPARSE arguments as the table below.\n",
    "\n",
    "\n",
    "| Param        | QSPARSE Argument |\n",
    "|--------------|-----------------------|\n",
    "| $N$          | `bits`                |\n",
    "| $t_q$        | `timeout`             |\n",
    "| $s$          | `sparsity`            |\n",
    "| $t_p$        | `start`               |\n",
    "| $n$          | `repetition`          |\n",
    "| $\\Delta t_p$ | `interval`            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `prune` and `quantize` layers maintain an internal counter to record the number of training steps that have passed through. The counter values can be accessed through the `_n_updates` attribute. Based on the above specified arguments, `conv.weight` will be quantized from step 100 and pruned with 50% sparsity from step 240, which can be verified by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Quantize] (channelwise) avg decimal = 8.0\n",
      "[Prune] [Step 210] active 0.72, pruned 0.28, window_size = 1\n",
      "[Prune] [Step 220] active 0.57, pruned 0.43, window_size = 1\n",
      "[Prune] [Step 230] active 0.51, pruned 0.49, window_size = 1\n",
      "[Prune] [Step 240] active 0.50, pruned 0.50, window_size = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([241], dtype=torch.int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.rand((1, 1, 32, 32))\n",
    "for _ in range(241):\n",
    "    conv(data)\n",
    "\n",
    "conv.quantize._n_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(conv.weight * (2**conv.quantize.decimal) \n",
    "- (conv.weight * (2**conv.quantize.decimal)).int()).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5034722222222222\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(conv.prune.mask.nonzero()) / np.prod(conv.prune.mask.shape))\n",
    "print(np.all((conv.weight.detach().numpy() == 0) \n",
    "          == (conv.prune.mask.detach().numpy() == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mask` and `decimal` denote the binary mask for pruning and number of fractional bits for quantization, which we will revisit in [Inspecting Parameters of a Pruned/Quantized Model](../advanced_usage/#inspecting-parameters-of-a-prunedquantized-model). The `prune` and `quantize` functions are compatible with any pytorch module as long as their parameters can be accessed from their `weight` attribute. Take another example of fully-connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(\n",
       "  in_features=128, out_features=10, bias=True\n",
       "  (prune): PruneLayer()\n",
       "  (quantize): QuantizeLayer()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantize(prune(nn.Linear(128, 10), 0.5), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Quantization and Pruning\n",
    "\n",
    "\n",
    "<figure style=\"text-align:center;font-style:italic\"> \n",
    "  <img src=\"../docs/assets/network_diagram-p3.svg\" />\n",
    "  <figcaption>The part of diagram in red corresponds to activation quantization and pruning.</figcaption>\n",
    "</figure>\n",
    "\n",
    "To prune and quantize and the output of a convolution, we can directly insert `quantize` and `prune` into the computation graph by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(\n",
       "    1, 32, kernel_size=(3, 3), stride=(1, 1)\n",
       "    (prune): PruneLayer()\n",
       "    (quantize): QuantizeLayer()\n",
       "  )\n",
       "  (1): PruneLayer()\n",
       "  (2): QuantizeLayer()\n",
       "  (3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(\n",
    "    conv,\n",
    "    prune(sparsity=0.5, start=200, interval=10, repetition=4),\n",
    "    quantize(bits=8, timeout=100),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the output of `conv` will be quantized from step 100 and pruned with 50% sparsity from step 240."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Network with Both Weight and Activation Quantized and Pruned\n",
    "\n",
    "Using the techniques introduced above, we can implement the `Net` so as to have joint quantization and pruning training capabilities with full transparency and minimal efforts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetPQ(\n",
       "  (qin): QuantizeLayer()\n",
       "  (conv1): Conv2d(\n",
       "    1, 32, kernel_size=(3, 3), stride=(1, 1)\n",
       "    (quantize): QuantizeLayer()\n",
       "  )\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (p1): PruneLayer()\n",
       "  (q1): QuantizeLayer()\n",
       "  (conv2): Conv2d(\n",
       "    32, 64, kernel_size=(3, 3), stride=(1, 1)\n",
       "    (prune): PruneLayer()\n",
       "    (quantize): QuantizeLayer()\n",
       "  )\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (p2): PruneLayer()\n",
       "  (q2): QuantizeLayer()\n",
       "  (fc1): Linear(\n",
       "    in_features=9216, out_features=128, bias=True\n",
       "    (prune): PruneLayer()\n",
       "    (quantize): QuantizeLayer()\n",
       "  )\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (p3): PruneLayer()\n",
       "  (q3): QuantizeLayer()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (quantize): QuantizeLayer()\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class NetPQ(nn.Module):  \n",
    "    def __init__(self, epoch_size=100):\n",
    "        super(NetPQ, self).__init__()\n",
    "        # input quantization, quantize at epoch 10\n",
    "        self.qin = quantize(bits=8, timeout=epoch_size * 10) \n",
    "\n",
    "        # For the sake of simplicity, we ignore the `timeout,start,repetition,\n",
    "        # interval` parameters in the following.\n",
    "        self.conv1 = quantize(nn.Conv2d(1, 32, 3, 1), 8)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.p1, self.q1 = prune(sparsity=0.5), quantize(bits=8)\n",
    "\n",
    "        self.conv2 = quantize(prune(nn.Conv2d(32, 64, 3, 1), 0.5), 8)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.p2, self.q2 = prune(sparsity=0.5), quantize(bits=8)\n",
    "\n",
    "        self.fc1 = quantize(prune(nn.Linear(9216, 128), 0.5), 8)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.p3, self.q3 = prune(sparsity=0.5), quantize(bits=8)\n",
    "\n",
    "        self.fc2 = quantize(nn.Linear(128, 10), 8)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.qin(x)                                         \n",
    "        x = F.relu(self.q1(self.p1(self.bn1(self.conv1(x)))))\n",
    "        x = F.relu(self.q2(self.p2(self.bn2(self.conv2(x)))))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.q3(self.p3(self.bn3(self.fc1(x)))))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)                                         \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "NetPQ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The network created by `NetPQ` is a pytorch module that only consists of its original components and `PruneLayer / QuantizeLayer` introduced by `prune` and `quantize`.\n",
    "It does not require you to modify the training loop or even the weight initialization code, and it also supports to [resume training from checkpoints](../advanced_usage/#resuming-from-checkpoint).\n",
    "\n",
    "The full example of training MNIST classifier with different pruning and quantization configurations can be found at [examples/mnist.py](https://github.com/mlzxy/qsparse/blob/main/examples/). More examples can be found in [here](https://github.com/mlzxy/qsparse-examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we introduce some basics about joint quantization and pruning training, and the implementation of this training paradigm with QSPARSE. Next, we introduce more [advanced usage](../advanced_usage/)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
